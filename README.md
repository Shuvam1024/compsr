# Super-resolution with compression
## Introduction
This project is about exploring use of deep convolutional neural networks for superresolution of images in the presence of compression and noise. In an increasingly popular and realistic image/video transmission scenario, images/videos can often be reduced in resolution and compressed before transmitting to a recipient. The recipient then decompresses the compressed bitstream and upscales to the display resolution before displaying it to a viewer. This process is happening anyway all the time when we view images on the web. The upscaling filters typically used on the receiver side are linear filters. In this project we want to investigate if and how much we can improve the quality of the final image if we used a deep Convolutional Neural Network (CNN) after the linear upscaling step. While the linear upscaling and CNN steps could be combined into one CNN superresolution step, for convenience of exploring down and upscaling ratios that are possibly fractional, in this project we keep the linear upscaling process separate from the CNN. The CNN thus operates on the linear upscaled image to produce a hopefully better quality image at the same resolution as its input.

While typical super-resolution processes deal with ratios that are 2:1 or higher, we consider down and upsampling ratios that are not just 2:1 but smaller and consequently fractional. Often the detail level at downsampling ratios higher than 2:1 are irrevocably lost, and therefore we focus on cases where there is a better chance to retrieve lost detail by a deep network. In particular we focus on the following down- and upsampling ratios: 2:1, 8:5 and 4:3.

Furthermore note that our CNN should recover from degradation induced not only by reduction in spatial resolution, but also from compression noise. Therefore, the quality of compression should be another factor that should be considered in the CNN. In particular, we focus on 4 compression levels: near-lossless-quality, high-quality, medium-quality, and low-quality, obtained with a modern video/image codec. The particular image codec we use is AV1 from the [Alliance for Open Media](https://aomedia.org/), even though extensions to other codecs is trivial. The QP values used with AV1 to generate the 4 compression levels are: 20 (near-lossless-quality), 30 (high-quality), 40 (medium-quality), and 50 (high-quality).

We envision a scenario where different CNNs may be used for different combinations of the pair: down/up ratio and compression-level. Therefore we generate training and testing data for each pair, which is 12 in all for 3 down-up ratios and 4 compression-levels. We hereby also release these datasets

Below, we provide the steps used to generate the datasets. All the datasets are adapted from the [Div2K](https://data.vision.ee.ethz.ch/cvl/DIV2K/) dataset originally released for a competition on super-resolution without compression. But we are happy to make our adaptations on this set available for free to other researchers in this space.
## Dataset Generation
The original Div2k dataset has 1000 images. Images numbered 1-800 comprised the training set, images 801-900 comprised the validation set, and images 901-1000 comprised the testing set. However, for 901-1000, only the low resolution set was released. For our purpose, we can only use the available full-resolution images. So we took the first 800 images (training set in the original Div2K set) and split them randomly into 640 for the training set (80%) and 160 for the validation set (20%). Then the images 801-900 (validation set in the original Div2K set) are used as the testing set in our pipeline. 

With this background here are the steps for generating each of the training, validation and testing datasets for down- and upsampling ratio r in {2:1, 8:5, 4:3} and quality parameter Q in {20, 30, 40, 50}.

* First convert each original RGB image in the set into YUV 420 format using ffmpeg. For image and video compression YUV 420 format is widely used, and further we only want to focus on improving the Y component for our CNN. U and V components are left to be processed using traditional linear down and upscaling. The YUV 420 image generated is referred to as the original image
* Use a Lanczos downscaler with parameter a = 5 to downscale each image by ratio r.
* Compress each downsampled image using the libaom software for AV1 with quality parameter Q.
* Decompress the bitstream generated to obtain a reconstructed version of the downsampled image.
* Use a Lanczos upscaler with parameter a = 5 to upscale the reconstructed image by ratio r. The resultant image must have the same size as the original image.
* Extract random patches of size p x p from the Y channel of the original image and the corresponding patch in the Y channel of the compressed upscaled image. Use a suitability criteria to make sure that the range of Y values (maximum - minimum) in the patch of the original image is at least 8.
* Store the patch pairs and the patch-size p in a compressed numpy npz format. See the Colab code for the exact format of the data.

Repeat the process for each of 12 {r, Q} pairs, each generating three datasets for training, validation and testing respectively. Note the patch-size p for testing does not need to be the same as that for training and validation since our network is fully convolutional. In these datasets, we have used 48 x 48 patches (p = 48) for training and validation, but 64 x 64 patches (p = 64) for testing. 

The datasets are available here:
* Ratio 2:1, Q = 20 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_train_2by1_20_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_valid_2by1_20_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_test_2by1_20_64x64.npz)
* Ratio 2:1, Q = 30 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_train_2by1_30_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_valid_2by1_30_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_test_2by1_30_64x64.npz)
* Ratio 2:1, Q = 40 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_train_2by1_40_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_valid_2by1_40_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_test_2by1_40_64x64.npz)
* Ratio 2:1, Q = 50 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_train_2by1_50_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_valid_2by1_50_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_2by1/DIV2K_test_2by1_50_64x64.npz)
* Ratio 8:5, Q = 20 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_train_8by5_20_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_valid_8by5_20_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_test_8by5_20_64x64.npz)
* Ratio 8:5, Q = 30 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_train_8by5_30_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_valid_8by5_30_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_test_8by5_30_64x64.npz)
* Ratio 8:5, Q = 40 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_train_8by5_40_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_valid_8by5_40_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_test_8by5_40_64x64.npz)
* Ratio 8:5, Q = 50 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_train_8by5_50_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_valid_8by5_50_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_8by5/DIV2K_test_8by5_50_64x64.npz)
* Ratio 4:3, Q = 20 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_train_4by3_20_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_valid_4by3_20_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_test_4by3_20_64x64.npz)
* Ratio 4:3, Q = 30 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_train_4by3_30_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_valid_4by3_30_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_test_4by3_30_64x64.npz)
* Ratio 4:3, Q = 40 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_train_4by3_40_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_valid_4by3_40_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_test_4by3_40_64x64.npz)
* Ratio 4:3, Q = 50 (Near-lossless quality): [Training](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_train_4by3_50_48x48.npz), [Validation](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_valid_4by3_50_48x48.npz), [Testing](https://storage.googleapis.com/srcompdata/Ratio_4by3/DIV2K_test_4by3_50_64x64.npz)
